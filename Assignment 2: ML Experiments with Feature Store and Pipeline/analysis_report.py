import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load results
results_df = pd.read_csv('experiment_results.csv')
emissions_df = pd.read_csv('emissions.csv')

# Set up the plotting style
plt.style.use('default')
sns.set_palette("husl")
fig = plt.figure(figsize=(20, 15))

# Add algorithm labels to emissions data
algorithm_labels = ['RF_Set1_V1', 'RF_Set2_V1', 'LR_Set1_V1', 'LR_Set2_V1',
                   'RF_Set1_V2', 'RF_Set2_V2', 'LR_Set1_V2', 'LR_Set2_V2']
emissions_df = emissions_df.copy()
emissions_df['experiment'] = algorithm_labels
emissions_df['emissions_mg'] = emissions_df['emissions'] * 1e9  # Convert to mg CO2

print("üéØ COMPREHENSIVE ANALYSIS REPORT")
print("=" * 60)

# 1. Model Performance Comparison (Quantitative)
plt.subplot(3, 3, 1)
results_pivot = results_df.pivot_table(values='accuracy', 
                                     index=['algorithm'], 
                                     columns=['feature_version'], 
                                     aggfunc='mean')
sns.heatmap(results_pivot, annot=True, fmt='.3f', cmap='Blues')
plt.title('Accuracy by Algorithm & Feature Version')
plt.ylabel('Algorithm')

plt.subplot(3, 3, 2)
results_pivot_f1 = results_df.pivot_table(values='f1', 
                                         index=['algorithm'], 
                                         columns=['feature_version'], 
                                         aggfunc='mean')
sns.heatmap(results_pivot_f1, annot=True, fmt='.3f', cmap='Greens')
plt.title('F1-Score by Algorithm & Feature Version')
plt.ylabel('Algorithm')

plt.subplot(3, 3, 3)
results_pivot_roc = results_df.pivot_table(values='roc_auc', 
                                          index=['algorithm'], 
                                          columns=['feature_version'], 
                                          aggfunc='mean')
sns.heatmap(results_pivot_roc, annot=True, fmt='.3f', cmap='Oranges')
plt.title('ROC-AUC by Algorithm & Feature Version')
plt.ylabel('Algorithm')

# 2. Detailed Metrics Comparison
plt.subplot(3, 3, 4)
metrics_data = []
for _, row in results_df.iterrows():
    metrics_data.extend([
        [row['algorithm'], row['feature_version'], 'Accuracy', row['accuracy']],
        [row['algorithm'], row['feature_version'], 'Precision', row['precision']],
        [row['algorithm'], row['feature_version'], 'Recall', row['recall']],
        [row['algorithm'], row['feature_version'], 'F1', row['f1']],
        [row['algorithm'], row['feature_version'], 'ROC-AUC', row['roc_auc']]
    ])

metrics_df = pd.DataFrame(metrics_data, columns=['Algorithm', 'Feature_Version', 'Metric', 'Value'])
sns.boxplot(data=metrics_df, x='Metric', y='Value', hue='Algorithm')
plt.title('Distribution of Metrics by Algorithm')
plt.xticks(rotation=45)

# 3. Feature Version Comparison
plt.subplot(3, 3, 5)
feature_comparison = results_df.groupby('feature_version')[['accuracy', 'f1', 'roc_auc']].mean()
feature_comparison.plot(kind='bar', ax=plt.gca())
plt.title('Average Performance by Feature Version')
plt.ylabel('Score')
plt.xticks(rotation=0)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

# 4. Carbon Emissions Analysis
plt.subplot(3, 3, 6)
plt.bar(range(len(emissions_df)), emissions_df['emissions_mg'])
plt.title('Carbon Emissions by Experiment (mg CO2)')
plt.xlabel('Experiment')
plt.ylabel('Emissions (mg CO2)')
plt.xticks(range(len(emissions_df)), emissions_df['experiment'], rotation=45, ha='right')

# 5. Performance vs Carbon Trade-off
plt.subplot(3, 3, 7)
colors = ['red' if 'RF' in exp else 'blue' for exp in emissions_df['experiment']]
plt.scatter(emissions_df['emissions_mg'], results_df['accuracy'], c=colors, s=100, alpha=0.7)
plt.xlabel('Carbon Emissions (mg CO2)')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Carbon Emissions Trade-off')
# Add legend
import matplotlib.patches as mpatches
red_patch = mpatches.Patch(color='red', label='RandomForest')
blue_patch = mpatches.Patch(color='blue', label='LogisticRegression')
plt.legend(handles=[red_patch, blue_patch])

# 6. Algorithm Hyperparameter Impact
plt.subplot(3, 3, 8)
rf_results = results_df[results_df['algorithm'] == 'RandomForest']
lr_results = results_df[results_df['algorithm'] == 'LogisticRegression']

x = np.arange(4)
width = 0.35
plt.bar(x - width/2, rf_results['f1'].values, width, label='RandomForest', alpha=0.7)
plt.bar(x + width/2, lr_results['f1'].values, width, label='LogisticRegression', alpha=0.7)
plt.xlabel('Hyperparameter Set & Feature Version')
plt.ylabel('F1-Score')
plt.title('F1-Score by Configuration')
plt.xticks(x, ['Set1_V1', 'Set2_V1', 'Set1_V2', 'Set2_V2'])
plt.legend()

# 7. Energy Consumption Breakdown
plt.subplot(3, 3, 9)
energy_data = emissions_df[['cpu_energy', 'ram_energy']].fillna(0)
energy_data.index = emissions_df['experiment']
energy_data.plot(kind='bar', stacked=True, ax=plt.gca())
plt.title('Energy Consumption Breakdown')
plt.ylabel('Energy (kWh)')
plt.xticks(rotation=45, ha='right')
plt.legend()

plt.tight_layout()
plt.savefig('comprehensive_analysis.png', dpi=300, bbox_inches='tight')
print("üìä Visualization saved as 'comprehensive_analysis.png'")

# Print detailed analysis
print("\nüìä QUANTITATIVE ANALYSIS:")
print("-" * 40)
best_acc_idx = results_df['accuracy'].idxmax()
best_f1_idx = results_df['f1'].idxmax()
min_emission_idx = emissions_df['emissions'].idxmin()

print(f"Best Overall Model: {results_df.loc[best_acc_idx, 'algorithm']} "
      f"({results_df.loc[best_acc_idx, 'feature_version']}) - "
      f"Accuracy: {results_df['accuracy'].max():.3f}")

print(f"Best F1-Score: {results_df.loc[best_f1_idx, 'algorithm']} "
      f"({results_df.loc[best_f1_idx, 'feature_version']}) - "
      f"F1: {results_df['f1'].max():.3f}")

print(f"Most Carbon Efficient: {emissions_df.loc[min_emission_idx, 'experiment']} - "
      f"Emissions: {emissions_df['emissions'].min():.2e} kg CO2")

print(f"Least Carbon Efficient: {emissions_df.loc[emissions_df['emissions'].idxmax(), 'experiment']} - "
      f"Emissions: {emissions_df['emissions'].max():.2e} kg CO2")

print("\nüîç QUALITATIVE INSIGHTS:")
print("-" * 40)
print("‚Ä¢ RandomForest consistently outperforms LogisticRegression across all metrics")
print("‚Ä¢ Feature Version 1 (physical attributes) achieves higher accuracy than Version 2")
print("‚Ä¢ V1 has larger training dataset (33,845) vs V2 (4,656) which impacts performance")
print("‚Ä¢ Hyperparameter tuning shows minimal impact on RandomForest performance")
print("‚Ä¢ LogisticRegression shows significant sensitivity to hyperparameter changes")
print("‚Ä¢ L1 penalty with low C value severely impacts LogisticRegression performance")
print(f"‚Ä¢ Carbon emissions vary significantly: {emissions_df['emissions'].min():.2e} to {emissions_df['emissions'].max():.2e} kg CO2")
print("‚Ä¢ RandomForest models generally consume more energy due to ensemble nature")

print("\nüèÜ MODEL RANKING BY PERFORMANCE:")
print("-" * 40)
ranking = results_df.sort_values('f1', ascending=False)[['algorithm', 'feature_version', 'f1', 'accuracy']].reset_index(drop=True)
for i, (_, row) in enumerate(ranking.iterrows(), 1):
    print(f"{i}. {row['algorithm']} ({row['feature_version']}) - F1: {row['f1']:.3f}, Acc: {row['accuracy']:.3f}")

print("\n‚úÖ Assignment Requirements Completed:")
print("-" * 40)
print("‚úì Feature Store implemented with Feast")
print("‚úì 2 Feature versions created and deployed")
print("‚úì 4 Model combinations trained (2 algos √ó 2 hyperparams)")
print("‚úì Carbon emissions tracked for all experiments")
print("‚úì Quantitative metrics calculated and compared")
print("‚úì Qualitative analysis and visualizations generated")
print("‚úì Performance vs carbon efficiency trade-offs analyzed")

plt.show()